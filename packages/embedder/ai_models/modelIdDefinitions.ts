import type {EmbeddingModelParams} from './AbstractEmbeddingsModel'

export const modelIdDefinitions = {
  'BAAI/bge-large-en-v1.5': {
    embeddingDimensions: 1024,
    precision: 32,
    maxInputTokens: 512,
    tableSuffix: 'bge_l_en_1p5',
    languages: ['en']
  },
  'llmrails/ember-v1': {
    embeddingDimensions: 1024,
    precision: 32,
    maxInputTokens: 512,
    tableSuffix: 'ember_1',
    languages: ['en']
  },
  'Qwen/Qwen3-Embedding-0.6B': {
    embeddingDimensions: 1024,
    precision: 16,
    // Actual max is 32768, but we half that because attention = seqlen**2, so RAM will go up 4x
    // Also, since we're using this for embeddings, we want chunks to have more meaning. No soup.
    maxInputTokens: 16384,
    tableSuffix: 'qwen3_600M',
    languages: [
      'af',
      'am',
      'ar',
      'as',
      'az',
      'be',
      'bg',
      'bn',
      'bo',
      'bs',
      'ca',
      'cs',
      'cy',
      'da',
      'de',
      'el',
      'en',
      'es',
      'et',
      'eu',
      'fa',
      'fi',
      'fo',
      'fr',
      'ga',
      'gd',
      'gl',
      'gu',
      'he',
      'hi',
      'hr',
      'ht',
      'hu',
      'hy',
      'id',
      'is',
      'it',
      'ja',
      'jv',
      'ka',
      'kk',
      'km',
      'kn',
      'ko',
      'lo',
      'lt',
      'lv',
      'mk',
      'ml',
      'mn',
      'mr',
      'ms',
      'mt',
      'my',
      'nb',
      'ne',
      'nl',
      'nn',
      'or',
      'pa',
      'pl',
      'ps',
      'pt',
      'ro',
      'ru',
      'sd',
      'si',
      'sk',
      'sl',
      'sq',
      'sr',
      'su',
      'sv',
      'sw',
      'ta',
      'te',
      'tg',
      'th',
      'tl',
      'tr',
      'tt',
      'uk',
      'ur',
      'uz',
      'vi',
      'zh'
    ]
  }
} satisfies Record<string, EmbeddingModelParams>

export type ModelId = keyof typeof modelIdDefinitions
